# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=slurmctld
SlurmctldAddr=slurmctld
#
#MailProg=/bin/mail
MpiDefault=none
#MpiParams=ports=#-#
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/slurmd.spool
SlurmUser=slurm
#SlurmdUser=root
StateSaveLocation=/var/spool/slurm/slurm.state
SwitchType=switch/none
TaskPlugin=task/affinity
#
#
# TIMERS
#KillWait=30
#MinJobAge=300
#SlurmctldTimeout=120
#SlurmdTimeout=300
#
#
# SCHEDULING
FastSchedule=1
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_Core
#
#
# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
ClusterName=prowave
#JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
#SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
#
GresTypes=gpu
#
#
# COMPUTE NODES
NodeName=node67 CPUs=8 Sockets=1 CoresPerSocket=4 ThreadsPerCore=2 Gres=gpu:2
NodeName=gpu68 CPUs=4 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 Gres=gpu:2
NodeName=gpu69 CPUs=8 Sockets=1 CoresPerSocket=4 ThreadsPerCore=2 Gres=gpu:3
PartitionName=prowave Nodes=gpu68 MaxTime=INFINITE State=UP Default=YES
PartitionName=prowave_cpu Nodes=gpu68 MaxTime=INFINITE State=UP
PartitionName=webmd Nodes=gpu69 MaxTime=INFINITE State=UP
PartitionName=webmd_cpu Nodes=gpu69 MaxTime=INFINITE State=UP
